<!DOCTYPE html>
<html>

<head>
    <title>01_markov_chain.md</title>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

html,footer,header{
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Custom MD PDF CSS
 */
html,footer,header{
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";

 }
body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<link rel="stylesheet" href="file:///home/uumami/itam/ia_p25/R%3A%5C2.Travail%5C1.Enseignement%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css"><link rel="stylesheet" href="file:///home/uumami/itam/ia_p25/D%3A%5Crdaros%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css">
</head>

<body>
    <blockquote>
<p>&quot;In life, good deeds often go unrewarded. However, with a large enough sample size, seemingly skewed outcomes will converge to an expected value, and things will eventually turn out in your favour.&quot;</p>
<p>— <em>Kaguya-sama: Love is War</em>, Chapter 195<br>
Aka Akasaka</p>
</blockquote>
<h1 id="markov-chains-in-ai-a-gentle-introduction">Markov Chains in AI: A Gentle Introduction</h1>
<h2 id="1-introducing-markov-chains">1. Introducing Markov Chains</h2>
<p>Markov chains are mathematical models that describe a sequence of events where the probability of each event (state) depends only on the state attained in the previous event – this is known as the <strong>Markov property</strong>, or <strong>memorylessness</strong>. In simple terms, the future state depends only on the <strong>present state</strong> and not on how that state was reached. Formally, if $s$ and $s'$ are states, a first-order Markov chain satisfies:</p>
<p>[ t(s, s') = P(s_{t+1} = s' \mid s_t = s), ]</p>
<p>meaning the chance of transitioning to state <em>s'</em> at the next time step $t+1$ given the current state $s_t$ is independent of past states before $t$. All such transition probabilities for a Markov chain can be organized into a <strong>transition matrix</strong> $T$, where each entry $T_{ij} = P(\text{next state} = j \mid \text{current state} = i)$. Each row of $T$ sums to 1 (a stochastic matrix), reflecting that from any state $i$, the probabilities of transitioning to all possible next states $j$ add up to 100%. The set of all possible states is called the <strong>state space</strong> (often denoted $S$).</p>
<p>To build intuition, consider a few relatable examples:</p>
<ul>
<li><strong>WhatsApp Replies:</strong> Whether you reply to a message might depend mostly on the latest message you saw (e.g. if it's a question or not) and not the entire chat history. This can be seen as a Markov chain where the state is &quot;last message type&quot; and influences your response (next state).</li>
<li><strong>Video Game AI:</strong> An NPC's behavior might be modeled with states like &quot;patrolling,&quot; &quot;alerted,&quot; or &quot;chasing.&quot; The choice of next action depends on its current state (if it's alert, it may transition to chasing with some probability).</li>
<li><strong>Social Media Trends:</strong> A topic on Twitter could be in states such as &quot;dormant,&quot; &quot;trending,&quot; or &quot;viral.&quot; Each day it might stay trending or fade, depending only on its current status.</li>
<li><strong>Dice Rolls and Traffic:</strong> A fair die roll is memoryless by nature (the next roll is independent of the last), and traffic flow at an intersection can be modeled so that the next minute's congestion depends mainly on the current level (assuming sudden accidents or earlier conditions beyond the current state are negligible).</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7f/Stochmatgraph.png" alt="Example of a Markov chain shown as a state transition diagram with three states"><br>
<em>Figure:</em> Example of a Markov chain shown as a state transition <strong>diagram</strong> with three states. Arrows indicate possible transitions between states, labeled by their transition probabilities. Such a diagram is an equivalent representation to a transition matrix, illustrating the Markov property: from each state (1, 2, or 3), the chain &quot;hops&quot; to the next state based only on the current node's outgoing probabilities, forgetting where it was earlier.</p>
<p><a href="https://commons.wikimedia.org/wiki/File:Stochmatgraph.png">Source: Wikimedia Commons - Stochmatgraph</a></p>
<p>In all these cases, the process can be thought of as a sequence of states $s_0 \to s_1 \to s_2 \to \cdots$ where each transition $s_t \to s_{t+1}$ follows a fixed probability rule $P(s_{t+1}\mid s_t)$ given by the matrix $T$. Because only the present matters, Markov chains dramatically simplify modeling sequential phenomena. Instead of tracking the entire history, we just keep track of the <strong>current state</strong>. This makes analysis and computation feasible for AI systems that must predict or plan over sequences, as we will see next.</p>
<h3 id="12-mathematical-form">1.2 Mathematical Form</h3>
<p>A <strong>Markov chain</strong> is a sequence of random variables ${s_t}_{t=0}^{\infty}$ taking values in a countable set $S$ (the <strong>state space</strong>). The defining property is that the probability of transitioning to the next state depends only on the current state and not on the states before that. Formally:</p>
<p>$P(s_{t+1} = j \mid s_t = i, s_{t-1} = i_{t-1}, \ldots, s_0 = i_0) = P(s_{t+1} = j \mid s_t = i),$</p>
<p>for all $i, j \in S$. This <strong>memorylessness</strong> is the <strong>Markov property</strong>.</p>
<h4 id="121-transition-probabilities-and-matrix">1.2.1 Transition Probabilities and Matrix</h4>
<p>For a <strong>discrete-time, finite</strong> Markov chain with finite state space $S = {1,2,\ldots,n}$, we define a <strong>transition probability</strong> $p_{ij}$ as</p>
<p>$p_{ij} = P(s_{t+1} = j \mid s_t = i).$</p>
<p>These probabilities form an $n \times n$ <strong>transition matrix</strong> $P = [p_{ij}]$ with two key properties:</p>
<ol>
<li><strong>Nonnegativity</strong>: $p_{ij} \ge 0$.</li>
<li><strong>Row-Stochastic</strong>: $\sum_{j=1}^n p_{ij} = 1$ for each $i$.</li>
</ol>
<p>Hence each row of $P$ corresponds to a probability distribution over next states. Because each row is a probability distribution, the matrix PP is sometimes called a row-stochastic matrix.</p>
<h3 id="13-example">1.3 Example</h3>
<p>Throughout this chapter, we will anchor discussions with a lighthearted example from <em>Kaguya-sama: Love is War</em>. Imagine three states describing the dynamic between Kaguya Shinomiya and Miyuki Shirogane:</p>
<ul>
<li><strong>Distanced (D)</strong></li>
<li><strong>Engaged (E)</strong></li>
<li><strong>Standoff (S)</strong></li>
</ul>
<p>Initially, we leave the transition probabilities abstract, simply noting that from D, they might stay in D or move to E or S. We will specify concrete probabilities in later sections to illustrate calculations (e.g., computing a steady-state distribution).</p>
<h2 id="2-designing-markov-chains">2. Designing Markov Chains</h2>
<h2 id="21-intuition">2.1 Intuition</h2>
<p>When designing a Markov chain model for an AI problem, the first step is to decide on a clear <strong>state representation</strong>. Each state should capture all the relevant information about the past that is needed to determine the future. If the environment is <strong>fully observable</strong> (the agent has access to the true state) and <strong>stochastic</strong> (there's randomness in transitions), we can often model its dynamics as a Markov chain. The transition probabilities can be estimated from data or defined from domain knowledge. Essentially, you ask: <em>&quot;Given state X at time t, what are the probabilities of moving to each possible state at time t+1?&quot;</em> Those probabilities form the entries of the transition matrix $T$. In a game of tic-tac-toe, for instance, a &quot;state&quot; might be the configuration of the board; if we were to randomize players' moves, we could form a Markov chain of game states. In a traffic system, a state could be the current traffic level (e.g. light, medium, heavy) and transitions model how traffic tends to build up or dissipate in the next hour.</p>
<p>Designing a Markov chain also involves understanding the <strong>environment's structure</strong>. In a fully observable environment, the agent knows exactly which state it's in, so it can directly use the Markov chain. In a partially observable setting, the agent might still <strong>assume</strong> an underlying Markov chain (this leads to Hidden Markov Models, touched on later). The environment's behavior can be <strong>deterministic or stochastic</strong>. If it's deterministic (the next state is fixed given the current state), the transition probabilities are 0 or 1; if it's stochastic, we have a distribution over next states. AI modelers often design Markov chains to approximate complex processes. For example, a simplified turn-based game like <em>Dungeons &amp; Dragons (D&amp;D)</em> might have states representing the turn order or phase of the game (exploration, battle, etc.), and probabilities capturing the likelihood of moving from one phase to another due to player actions or dice rolls.</p>
<p>In game theory and multi-agent settings, Markov chains can model evolving <strong>game states or player strategies</strong>. Consider a competitive video game with two players: one could model the state of the match (who is winning, special conditions active, etc.) as a Markov chain, where transitions occur based on the combined strategies and randomness. Even though players choose actions, if we fix a policy for each player or use a mixed (probabilistic) strategy, the <em>resulting state evolution</em> can be seen as a Markov chain. For instance, in a simplified rock-paper-scissors tournament, the state might be the score difference, and given the current difference, the match might swing to a win, loss, or tie with certain probabilities (assuming certain strategy patterns). Markov chains are useful in such scenarios to analyze long-run behavior or the likelihood of states (like eventually winning the game from a given situation).</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/9/9a/Transition_graph_pac-man.png" alt="State transition diagram for Pac-Man"><br>
<em>Figure:</em> State transition diagram for a video game (Pac-Man as an example). Each oval is a game state (numbered regions in a Pac-Man grid) and each arrow shows a possible state transition with a probability. For example, from state 5 the game might move to state 6 with 1/3 probability, or back to state 8 with 1/4, etc., based on Pac-Man's movement and random ghost behavior. Such diagrams help in <strong>designing Markov chains</strong> by visualizing how an agent (like Pac-Man) can move through the state space. In designing your Markov model, you'd ensure that from any given state (node), the outgoing arrows and their probabilities accurately reflect the environment's rules. By carefully choosing states and transition probabilities, we create a Markov chain model that captures the essential dynamics of the system or game.</p>
<p><a href="https://commons.wikimedia.org/wiki/File:Transition_graph_pac-man.png">Source: Wikimedia Commons - Transition graph pac-man</a></p>
<h3 id="22-formal-definition">2.2 Formal Definition</h3>
<h3 id="221-state-space-and-observability">2.2.1 State Space and Observability</h3>
<p>A crucial modeling step is choosing what constitutes a <strong>state</strong> so that it encapsulates all the relevant information for predicting the next state. In a <strong>fully observable</strong> environment, the current state is fully known; if we assume Markovian dynamics, then we only need that one state to predict the next. If states are only partially observable, the model becomes a <strong>Hidden Markov Model</strong>.</p>
<ol>
<li><strong>State Definition</strong>: Ensure each state is well-defined and comprehensive enough that the Markov property realistically holds.</li>
<li><strong>Deterministic vs. Stochastic</strong>: Deterministic environments yield transition matrices with 0s and 1s; stochastic environments have rows that are &quot;spread&quot; over multiple possible next states.</li>
</ol>
<h3 id="222-estimating-transition-probabilities">2.2.2 Estimating Transition Probabilities</h3>
<p>If data is available, one estimates $p_{ij}$ from observed transitions:</p>
<p>$p_{ij} \approx \frac{\text{#(times the chain transitioned from state }i\text{ to }j)} {\text{#(times the chain was observed in state }i\text{)}}.$</p>
<p>Where data is sparse, smoothing methods or domain knowledge can help define probabilities.</p>
<h3 id="23-example">2.3 Example</h3>
<p>Suppose we hypothesize:</p>
<ul>
<li>From <strong>Distanced (D)</strong>: 50% chance they remain <strong>Distanced</strong>, 30% chance they become <strong>Engaged</strong>, 20% chance a <strong>Standoff</strong> arises.</li>
<li>From <strong>Engaged (E)</strong>: 20% chance to step back to <strong>Distanced</strong>, 40% chance to stay <strong>Engaged</strong>, 40% chance to escalate to <strong>Standoff</strong>.</li>
<li>From <strong>Standoff (S)</strong>: 10% chance to move to <strong>Distanced</strong>, 40% chance to revert to <strong>Engaged</strong>, and 50% chance remain in <strong>Standoff</strong>.</li>
</ul>
<p>Then the transition matrix is:</p>
<p>$P = \begin{bmatrix}<br>
0.5 &amp; 0.3 &amp; 0.2\<br>
0.2 &amp; 0.4 &amp; 0.4\<br>
0.1 &amp; 0.4 &amp; 0.5<br>
\end{bmatrix},$</p>
<h2 id="3-computing-markov-chains">3. Computing Markov Chains</h2>
<h3 id="31-intuition">3.1 Intuition</h3>
<p>Once we have a Markov chain defined (state space and transition matrix $T$), we can perform various computations to understand its behavior. A central concept is the <strong>steady-state distribution</strong> (also called the <em>stationary distribution</em> $\pi$). This is a probability distribution over states that remains <em>unchanged</em> by the transition process – in other words, $\pi$ satisfies $\pi T = \pi$. If you start the chain in the steady-state distribution, it will stay in that distribution at all times. Intuitively, as the chain runs for a long time, it &quot;forgets&quot; its initial state and the fraction of time it spends in each state converges to the steady-state probabilities. For example, if a particular state has a steady-state probability of 0.25, then in the long run the chain will be in that state about 25% of the time (on average). Finding $\pi$ involves solving a system of linear equations (one for each state), or by computing $T^n$ for large $n$ and looking at where the rows (or columns) stabilize.</p>
<p>To compute <strong>n-step transition</strong> probabilities (the probability of going from state $i$ to state $j$ in exactly $n$ steps), we can take the matrix power $T^n$. The $(i,j)$ entry of $T^n$ gives $P(s_n = j \mid s_0 = i)$. This is useful for questions like &quot;If the system is in state A now, what's the probability it will be in state B after 5 transitions?&quot; – you'd look at $(T^5)_{AB}$. Computationally, one can do this by repeated matrix multiplication or using algorithms like <strong>power iteration</strong> which repeatedly multiplies an initial distribution by $T$ (simulating the chain) until convergence. Power iteration is a simple way to approximate the steady-state: start with any guess $\pi^{(0)}$ and compute $\pi^{(k+1)} = \pi^{(k)} T$ at each step; as $k$ grows, $\pi^{(k)}$ will approach the steady-state $\pi$ for an <em>ergodic</em> chain.</p>
<p><strong>Ergodicity</strong> is a property that ensures a Markov chain will converge to a unique steady-state distribution. An ergodic chain is one that is both <strong>irreducible</strong> (it's possible to eventually reach any state from any other state, at least in theory) and <strong>aperiodic</strong> (it doesn't get caught in cycles with a fixed period). If these conditions hold, $T^n$ as $n \to \infty$ approaches a matrix where each row is the steady-state $\pi$. Many real-world processes have this tendency of &quot;forgetting&quot; initial conditions if they run long enough and if no state is absorbing. For example, consider web page surfing as a Markov chain: given some reasonable randomness (the basis of Google's PageRank algorithm), eventually the probability of being on any given page stabilizes (that's a steady-state).</p>
<p>Other computations of interest include <strong>expected hitting times</strong> (e.g., &quot;how many steps on average to eventually reach state X starting from state Y?&quot;) and <strong>absorption probabilities</strong> in chains that have absorbing states (states that once entered cannot be left). These can often be derived from $T^n$ or by solving systems of linear equations. In summary, by leveraging linear algebra (matrix powers, eigenvalues, eigenvectors) we can analyze Markov chains quantitatively. For instance, steady-state probabilities correspond to an eigenvector of $T$ (specifically, the left eigenvector with eigenvalue 1). Tools like eigenvalue decomposition give insight into the chain's long-term behavior and how fast it converges to steady-state (the second-largest eigenvalue magnitude relates to the convergence rate).</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Markovkate_01.svg" alt="Two-state Markov chain"><br>
<em>Figure:</em> A simple two-state Markov chain (State A and State E). Arrows indicate the probability of transitioning from one state to the other (e.g., the arrow from A to E might have probability $p$, and from A to A is $1-p$). In such a chain, it's easy to compute the steady-state by hand – it will be the ratio of the opposing transition probabilities. For example, if $P(A \to E) = P(E \to A) = 0.5$, the steady-state distribution is 50% A and 50% E. In more complex chains, we use algorithms to find this equilibrium.</p>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain">Source: Wikipedia - Markov chain</a></p>
<h3 id="32-formal-definition">3.2 Formal definition</h3>
<p>Once a Markov chain is defined, several fundamental properties and quantities are of great interest: <strong>n-step transition probabilities</strong>, <strong>steady-state (stationary) distributions</strong>, <strong>ergodicity</strong>, <strong>hitting times</strong>, <strong>absorption probabilities</strong>, and <strong>eigenvalue-based convergence</strong>. Below, we present formal definitions and detailed numerical examples to illustrate each concept thoroughly.</p>
<h4 id="321-n-step-transition-probabilities">3.2.1 n-Step Transition Probabilities</h4>
<p>Define $P^n$ as the $n$-th matrix power of $P$. The entry $(P^n)<em>{ij}$ gives<br>
$$(P^n)</em>{ij} = P(s_n=j \mid s_0=i).$$</p>
<p>Hence, $(P^2)_{ij}$ yields the probability of going from $i$ to $j$ in 2 steps, etc.</p>
<p><strong>Why does matrix powering work?</strong> Because the probability of a <em>length-n</em> path factors into the product of transition probabilities along the path. When you multiply $P$ by itself, you sum over all intermediate possibilities, systematically accounting for every path of <em>length-n</em>.</p>
<p><strong>Example:</strong> In the 3-state Kaguya-Shirogane chain $(D,E,S)$, computing $P^2$ by matrix multiplication gives you 2-step transition probabilities. For instance, if from Distanced to Standoff in 2 steps is $(P^2)_{D,S}$, you can see how likely it is for Kaguya and Shirogane to escalate from being Distanced now to a Standoff 2 interactions later.</p>
<h4 id="322-steady-state-stationary-distribution">3.2.2 Steady-State (Stationary) Distribution</h4>
<p>A stationary distribution $\pi$ (a row vector) satisfies:</p>
<ul>
<li>$\pi P = \pi$,</li>
<li>$\sum_{i \in S} \pi_i = 1$, $\pi_i \geq 0$.</li>
</ul>
<p>Equivalently, $\pi$ is the left eigenvector of $P$ associated with the eigenvalue $1$. Interpreting $\pi$ as a probability distribution over states, condition $\pi P = \pi$ means that if the chain is &quot;started&quot; in $\pi$, it remains in $\pi$ at each step.</p>
<ul>
<li><strong>Left Eigenvector</strong>: Algebraically, $\pi$ being a solution to $\pi P = \pi$ means $\pi$ is a <strong>left eigenvector</strong> of $P$ with eigenvalue $1$.</li>
<li><strong>Intuition</strong>: If $\pi$ is your current &quot;distribution&quot; of states, multiplying by $P$ doesn't change it. You can see $\pi$ as a balance of &quot;inflows&quot; and &quot;outflows&quot; for each state: the fraction of probability that arrives into state $i$ from other states equals what leaves state $i$. That self-consistency yields no net change. Another way to see it, if you view probabilities as flows along edges in the Markov chain, each state’s incoming flow equals its outgoing flow under ππ. This makes ππ an equilibrium point—no net movement occurs in that distribution.</li>
<li><strong>Existence and Uniqueness</strong>: If the chain is <strong>irreducible</strong> and <strong>aperiodic</strong> (see 3.2.4), then $\pi$ is unique and is called the <strong>steady-state</strong>. If the chain is not irreducible, multiple stationary distributions might exist or they might be confined to subsets of states.</li>
<li><strong>Practical Computation</strong>: Often, one solves $\pi P=\pi, \sum_i\pi_i=1$ via linear algebra. Alternatively, <strong>power iteration</strong> repeatedly multiplies an initial vector by $P$ until convergence, which also yields $\pi$.</li>
</ul>
<h4 id="323-existence-and-uniqueness">3.2.3 Existence and Uniqueness</h4>
<ul>
<li>A finite Markov chain may have more than one stationary distribution if it is not irreducible (i.e., if the state space is broken into multiple &quot;communication classes&quot;).</li>
<li>If the chain is irreducible and aperiodic (see Section 3.2.4), there is exactly one stationary distribution $\pi$, often called the steady-state distribution.</li>
</ul>
<p><strong>Interpretation:</strong> If you run an irreducible, aperiodic Markov chain for a long time, the fraction of time it spends in each state converges to $\pi$, regardless of the initial state. This is sometimes referred to colloquially as &quot;the chain forgets its initial condition.&quot;</p>
<h5 id="3231-computing-pi-%E2%80%93-numerical-example">3.2.3.1 Computing $\pi$ – Numerical Example</h5>
<p>Using the Kaguya-Shirogane transition matrix:<br>
$$P = \begin{bmatrix}<br>
0.5 &amp; 0.3 &amp; 0.2\<br>
0.2 &amp; 0.4 &amp; 0.4\<br>
0.1 &amp; 0.4 &amp; 0.5<br>
\end{bmatrix},$$</p>
<p>we want to find $\pi = (\pi_D, \pi_E, \pi_S)$ such that:<br>
$$\pi P = \pi, \quad \pi_D + \pi_E + \pi_S = 1, \quad \pi_i \geq 0.$$</p>
<p><strong>System of Equations</strong></p>
<p>Writing $\pi P = \pi$ explicitly:<br>
$$\begin{cases}<br>
\pi_D = 0.5 \pi_D + 0.2 \pi_E + 0.1 \pi_S,\<br>
\pi_E = 0.3 \pi_D + 0.4 \pi_E + 0.4 \pi_S,\<br>
\pi_S = 0.2 \pi_D + 0.4 \pi_E + 0.5 \pi_S,\<br>
\pi_D + \pi_E + \pi_S = 1.<br>
\end{cases}$$</p>
<p>We can solve by standard linear algebra methods (e.g., subtract each row from $\pi_i$, then add the normalization constraint). A quick method is also power iteration:</p>
<ol>
<li>Choose an initial $\pi^{(0)}$, say $(1/3, 1/3, 1/3)$.</li>
<li>Repeatedly compute $\pi^{(k+1)} = \pi^{(k)} P$.</li>
<li>Iterate until $\pi^{(k+1)} \approx \pi^{(k)}$ within a tolerance $\varepsilon$.</li>
</ol>
<p><strong>Illustrative Result</strong></p>
<p>One finds a unique $\pi \approx (0.326, 0.352, 0.322)$. This means that in the long run, the chain spends roughly 32.6% of the time in Distanced, 35.2% in Engaged, and 32.2% in Standoff—no matter where it started (as long as the chain is irreducible and aperiodic).</p>
<h4 id="324-ergodicity-irreducibility-and-aperiodicity">3.2.4 Ergodicity: Irreducibility and Aperiodicity</h4>
<p><strong>Definition:</strong> A Markov chain is ergodic if it converges to a unique steady-state distribution from any initial state distribution. Two properties ensure ergodicity in a finite chain:</p>
<ol>
<li><strong>Irreducible:</strong> The chain is irreducible if for all states $i$ and $j$, there exists some integer $n$ such that $(P^n)_{ij} &gt; 0$. In other words, any state is reachable from any other state (in some number of steps).</li>
<li><strong>Aperiodic:</strong> The period of a state $i$ is the greatest common divisor (gcd) of all $n$ such that $(P^n)_{ii} &gt; 0$. A state is aperiodic if its period is 1. A chain is aperiodic if all its states are aperiodic.</li>
</ol>
<p><strong>Theorem (Ergodic Theorem)</strong></p>
<p>If a finite Markov chain is irreducible and aperiodic, then there is a unique stationary distribution $\pi$. Moreover,<br>
$$\lim_{n \to \infty} P^n = \begin{bmatrix} \pi \ \pi \ \vdots \ \pi \end{bmatrix},$$</p>
<p>meaning every row of $P^n$ converges to $\pi$ as $n \to \infty$.</p>
<h5 id="ergodic-theoremmeaning--importance">Ergodic Theorem—Meaning &amp; Importance</h5>
<ul>
<li><strong>Exact Statement</strong>: $\displaystyle \lim_{n\to\infty}P^n$ has each row equal to the unique steady-state vector $\pi$.</li>
<li><strong>Intuition</strong>: From any initial state distribution, repeated applications of $P$ &quot;mix&quot; the chain so thoroughly that the past is &quot;forgotten.&quot; Eventually, you stabilize to the same distribution $\pi$ regardless of how you began. Because the chain is irreducible, there's only one &quot;pool&quot; of states, and being aperiodic prevents the chain from cycling with a fixed interval. Over many steps, any initial distribution &quot;mixes&quot; across all states until it stabilizes at $\pi$. Thus, the row $(P^n)_{i,\cdot}$ (the distribution after $n$ steps from state $i$) converges to $\pi$ for every $i$.</li>
<li><strong>Importance</strong>: This allows us to discuss <strong>long-run behavior</strong> unambiguously: e.g., in the Kaguya-Shirogane chain, the fraction of time in each rivalry state &quot;converges&quot; to a single set of proportions. It also underlies algorithms like PageRank, where the chain's &quot;steady-state&quot; reveals the page importance.</li>
<li><strong>Who made it?</strong>: John von Neuman considered it one if his greatest contributions to science.</li>
</ul>
<h4 id="325-hitting-times">3.2.5 Hitting Times</h4>
<p>Let $T_j$ be the <strong>first time</strong> the chain visits state $j$. Formally:</p>
<p>$T_j = \min\{n \ge 0 : s_n = j\}$.</p>
<p>If our chain starts in state $i \neq j$, then $T_j$ measures <em>how many steps</em> it takes (from time zero onward) to reach $j$ for the first time. We define the <strong>expected hitting time</strong> as</p>
<p>$h_i = E[T_j|s_0 = i]$.</p>
<h5 id="why-the-standard-formula">Why the Standard Formula?</h5>
<ul>
<li>If $i = j$, clearly $h_j = 0$ because we are already in state $j$.</li>
<li>If $i \neq j$, in one step we move to some state $k$ with probability $p_{ik}$.
<ul>
<li>If $k = j$, we have arrived after exactly 1 step (no more steps needed).</li>
<li>If $k \neq j$, we still need $h_k$ additional steps on average from $k$.</li>
</ul>
</li>
</ul>
<p>Hence we write:</p>
<p>$h_j = 0,$</p>
<p>$h_i = 1 + \sum_{k \neq j} p_{ik}(h_k), \text{for } i \neq j.$</p>
<p>The &quot;1&quot; accounts for the step we just took from $i$. After that, with probability $p_{ik}$, we either finish if $k = j$ or continue for $h_k$ more steps if $k \neq j$. Summing over all $k$ weighted by $p_{ik}$ yields a self-consistent linear equation for $h_i$.</p>
<h5 id="mathematical-intuition">Mathematical Intuition</h5>
<ol>
<li><strong>Linearity of Expectation</strong>: We decompose the expected number of steps into &quot;1 step taken now&quot; + &quot;expected steps remaining afterward.&quot;</li>
<li><strong>System of Equations</strong>: The unknowns $\{h_i\}$ are linked by these equations; solving them simultaneously gives each $h_i$.</li>
<li><strong>Interpretation</strong>: $h_i$ is the average time from $i$ to first visit $j$. In reliability engineering, $j$ might be &quot;failure.&quot; In AI planning, $j$ might be a &quot;goal&quot; state. Hitting times thus quantify how long we expect before hitting a key event.</li>
</ol>
<h5 id="3251-numeric-example-of-hitting-time">3.2.5.1 Numeric Example of Hitting Time</h5>
<p>Consider a smaller 2-state chain to keep the computation simple:<br>
$$P = \begin{bmatrix} 0.6 &amp; 0.4 \ 0.2 &amp; 0.8 \end{bmatrix},$$</p>
<p>with states ${1,2}$. Suppose we want the expected time to hit state 2 starting from state 1. Let $h_1$ denote $E[T_2 \mid s_0 = 1]$. Then:</p>
<ul>
<li>$h_2 = 0$ by definition (if you start in state 2, the hitting time of 2 is zero).</li>
<li>For state 1:<br>
$h_1 = 1 + 0.6 h_1 + 0.4 h_2$,<br>
but $h_2 = 0$<br>
Rearranging:<br>
$h_1 - 0.6 h_1 = 1$,<br>
$0.4 h_1 = 1$,<br>
$h_1 = \frac{1}{0.4} = 2.5$</li>
</ul>
<p>Thus, starting in state 1, on average it takes 2.5 steps to reach state 2.</p>
<p>One can analogously define and solve for $E[T_1 \mid s_0 = 2]$.</p>
<h4 id="326-absorbing-states-and-absorption-probabilities">3.2.6 Absorbing States and Absorption Probabilities</h4>
<p>A state $a$ is absorbing if $p_{aa} = 1$. Once entered, it cannot be left. For instance, if Kaguya and Shirogane ultimately confess and become a couple, that state might be &quot;absorbing&quot; in our story's simplified chain model: once there, they do not revert to the old rivalry states.</p>
<p><strong>Canonical Form:</strong> When analyzing absorbing states, we can reorder states so that the transition matrix has the block form:<br>
$$P = \begin{bmatrix} Q &amp; R \ 0 &amp; I \end{bmatrix},$$</p>
<p>where</p>
<ul>
<li>$Q$ (dimension $t \times t$) is transitions among transient states.</li>
<li>$R$ (dimension $t \times r$) is transitions from transient to absorbing states.</li>
<li>$I$ (dimension $r \times r$) is the identity, reflecting that absorbing states never change.</li>
</ul>
<h5 id="3261-fundamental-matrix-and-absorption-probabilities">3.2.6.1 Fundamental Matrix and Absorption Probabilities</h5>
<p>The fundamental matrix is $N = (I - Q)^{-1}$. If you label transient states as $1, \ldots, t$ and absorbing states as $t+1, \ldots, n$, then the matrix of absorption probabilities is:<br>
$$B = N R.$$</p>
<p>Algebraically, you can view $N$ as $(I + Q + Q^2 + \dots)$, a convergent series if no transient state traps you forever. * <strong>Why $(I-Q)^{-1}$?</strong> Intuitively, each power $Q^m$ describes the probability of staying among transient states for $m$ steps. Summing across all $m$ captures the total expected number of visits to each transient state. Thus, $N_{ik}$ is how many times, on average, you visit state $k$ if you start in state $i$ <strong>before</strong> absorption.</p>
<p>$B = N ,R$, a $t \times r$ matrix. Each entry $B_{ij}$ gives the probability that, starting in transient state $i$, you eventually end up in absorbing state $j$. The logic is: once you know how many times you expect to be in each transient state (from $N$), you multiply by $R$ to see where you go when you leave the transient set.</p>
<h5 id="3262-kaguya-shirogane-example-with-two-absorbing-states">3.2.6.2 Kaguya-Shirogane Example With Two Absorbing States</h5>
<p>Extend the state set to ${D, E, S, B, X}$:</p>
<ul>
<li>$D, E, S$ are transient (Distanced, Engaged, Standoff).</li>
<li>$B$ = &quot;Breakthrough&quot;: They confess or reach a conclusive relationship. (Absorbing)</li>
<li>$X$ = &quot;Deadlock&quot;: Pride overwhelms them; they quit interacting meaningfully. (Absorbing)</li>
<li>$I$ = &quot;Identity Matrix&quot;: Is the identity matrix with the dimention requiered for the operation.</li>
</ul>
<p>A possible $5 \times 5$ transition matrix (in canonical order $[D,E,S \mid B,X]$):<br>
$$P = \begin{bmatrix}<br>
0.4 &amp; 0.4 &amp; 0.2 &amp; 0 &amp; 0 \<br>
0.2 &amp; 0.5 &amp; 0.3 &amp; 0 &amp; 0 \<br>
0.1 &amp; 0.3 &amp; 0.4 &amp; 0.2 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br>
\end{bmatrix},$$</p>
<p>so<br>
$$Q = \begin{bmatrix}<br>
0.4 &amp; 0.4 &amp; 0.2 \<br>
0.2 &amp; 0.5 &amp; 0.3 \<br>
0.1 &amp; 0.3 &amp; 0.4<br>
\end{bmatrix}, \quad<br>
R = \begin{bmatrix}<br>
0 &amp; 0 \<br>
0 &amp; 0 \<br>
0.2 &amp; 0<br>
\end{bmatrix}.$$</p>
<p>Then compute $N = (I - Q)^{-1}$ and $B = N R$. The resulting $B$ will show how likely it is for the chain to end in $B$ (Breakthrough) vs. $X$ (Deadlock) from each of $D, E, S$.</p>
<h4 id="327-eigenvalues-and-convergence-rates">3.2.7 Eigenvalues and Convergence Rates</h4>
<p>For an $n \times n$ transition matrix $P$:</p>
<ul>
<li>The largest eigenvalue is always 1.</li>
<li>All other eigenvalues lie in the closed unit disk (by the Perron–Frobenius theorem for stochastic matrices).</li>
</ul>
<p>If the chain is irreducible and aperiodic, the eigenvalue 1 is simple (i.e., has algebraic multiplicity 1), and the corresponding eigenvector is the unique $\pi$. The second-largest eigenvalue in magnitude determines the rate of convergence to $\pi$. Specifically, if $\lambda_2$ is the eigenvalue with largest magnitude below 1, then $|P^n - J|$ (where $J$ is the matrix of repeated rows $\pi$) decreases roughly on the order of $\lambda_2^n$.</p>
<p>If the chain is <strong>ergodic</strong>, that eigenvalue is unique in magnitude—other eigenvalues have $|\lambda| &lt; 1$.</p>
<p><strong>Why does $|\lambda| &lt; 1$ force convergence?</strong> When you repeatedly multiply by $P$, any component tied to an eigenvalue $\lambda$ with $|\lambda|&lt;1$ shrinks to zero as $n\to\infty$. Only the part aligned with the eigenvalue 1 remains. In Markov chains, that part corresponds to the stationary distribution $\pi$. Hence, $P^n\to$ a matrix of repeated rows $\pi$.</p>
<p><strong>Practical Impact</strong>: A larger gap between 1 and the second-largest eigenvalue typically means faster convergence. In algorithms like PageRank, ensuring all other eigenvalues are strictly below 1 in magnitude speeds up the approach to the steady state.</p>
<h2 id="4-inference-and-ai-agents">4. Inference and AI Agents</h2>
<p>Markov chains provide a foundation for how AI agents <em>infer</em> the dynamics of their environment and make decisions. If an AI agent assumes the world follows the Markov property, it can simplify its decision-making by focusing on the current state. Different types of agents use Markov chain reasoning in various ways:</p>
<ul>
<li><strong>Simple Reflex Agents:</strong> These agents choose actions based only on the current state, ignoring history. This aligns perfectly with the Markov property. If the agent's rule says &quot;if in state S, do action A,&quot; it's effectively leveraging the transition probability $t(S, \cdot)$ (even if implicitly) to react. For example, a thermostat is a simple reflex agent – it doesn't care whether it was hot an hour ago, only whether it's hot <em>now</em> (current state) to decide turning the AC on. We can model the room temperature as a Markov chain where the next temperature depends only on the current temperature and the AC/heater actions.</li>
<li><strong>Model-Based Reflex Agents:</strong> These have an internal <strong>model</strong> of the world's transition dynamics (a model of $t(s,s')$). In other words, the agent builds or is given a Markov chain of the environment. It can predict next states: &quot;If I'm in state S now, my model says there's a 70% chance I go to S' and 30% chance to S'' after one step.&quot; This is essentially using the transition matrix $T$ for inference. For instance, a game AI might know the Markovian rules of the game (like how opponents usually behave from the current game state) and can update its beliefs about what will happen one turn ahead.</li>
<li><strong>Goal-Based Agents:</strong> These agents consider future sequences of states to achieve a goal. They can use Markov chain properties to plan by looking ahead multiple steps (often this is done with search or planning algorithms). If the agent knows the transition matrix $T$, it can compute $T^n$ or simulate many transitions to see where it might end up. Goal-based reasoning often involves finding a sequence of transitions (a path through the Markov chain) that leads to a <strong>goal state</strong>. In Markov terms, the agent might be interested in the probability of eventually reaching a goal state (a hitting probability) or the expected number of steps to reach it from the current state. By examining the chain (sometimes using algorithms like dynamic programming if rewards are involved), the agent can choose actions that increase the likelihood of reaching the goal. In a path-planning AI for navigation, for example, the map can be viewed as a Markov chain of locations, and the goal-based agent searches for a high-probability path to the destination.</li>
<li><strong>Utility-Based Agents:</strong> These agents not only have goals but also a utility (value) function to evaluate how desirable each state is. When an environment is modeled as a Markov chain, a utility-based agent will combine the transition probabilities with the utilities of future states to decide the best action. Essentially, it performs a kind of <em>expectimax</em> or expectated utility calculation: it considers &quot;if I am in state S, and I take action A, the next state could be S' or S'' with certain probabilities (from the Markov model); how much utility will I likely get?&quot; The agent will prefer the action that leads to the highest expected utility. In practice, this often leads into the realm of <strong>Markov Decision Processes (MDPs)</strong> – which are Markov chains extended with actions and rewards. The utility-based agent, via algorithms like value iteration or policy iteration, calculates the optimal policy on an underlying Markov chain of states.</li>
<li><strong>Learning Agents:</strong> These agents improve over time by learning from experience. In the context of Markov chains, a learning agent might start with unknown transition probabilities and then <strong>learn</strong> them by observing the frequencies of state transitions (essentially doing empirical estimation of $t(s,s')$). This is exactly what happens in <strong>reinforcement learning</strong>, where an agent explores an environment and estimates the state transition model and/or the value of states. By assuming the environment is Markovian, the learning algorithms (like Q-learning or Monte Carlo simulation) update estimates based on the current state and the observed next state, gradually converging to an accurate model of the Markov chain. Learning agents connect game theory and Markov chains when, for example, they adjust their strategy in a game (the transition probabilities might change as the opponent learns too – a more complex scenario!). Nonetheless, many AI learning scenarios use the Markov assumption as a backbone; even if the agent doesn't explicitly learn &quot;the matrix,&quot; it often learns a policy that is optimal for the Markov chain dynamics of the environment.</li>
</ul>
<p>It's worth noting how Markov chains are the stepping stone to more advanced models in AI:</p>
<ul>
<li>A <strong>Hidden Markov Model (HMM)</strong> is essentially a Markov chain where the states are not directly observed (they're &quot;hidden&quot;) and we only see some observations emitted from states. The underlying chain might model, say, the emotional state of a user (happy, neutral, sad) which is not directly observable, while the observations are their messages or actions. The AI must infer the hidden state distribution using algorithms like forward-backward – all built on the Markov assumption for state transitions.</li>
<li>A <strong>Markov Decision Process (MDP)</strong>, as mentioned, is a Markov chain augmented with actions and rewards. In an MDP, when an agent takes a given action in state $s$, the state transitions according to $P(s_{t+1}=s' \mid s_t=s, a_t=a)$. This still satisfies the Markov property (next state depends only on current state and action), and forms the core of how we model decision-making problems for AI. Solving an MDP (via dynamic programming, reinforcement learning, etc.) essentially means finding a good policy assuming the environment's transitions are a Markov chain influenced by actions.</li>
<li>A <strong>Partially Observable MDP (POMDP)</strong> generalizes HMMs and MDPs – here the agent doesn't directly observe the true state, so it keeps a <strong>belief</strong> (a probability distribution over states). The belief itself can be seen as a state in an <em>even larger</em> Markov process! Markov chains thus even underlie the belief-state dynamics: given the current belief (distribution over where we might be), and an action and observation, we update to a new belief (this belief update can be seen as a transition in a belief-state Markov chain).</li>
</ul>
<p>In summary, Markov chains provide the reasoning framework for AI agents to predict and plan. They allow agents to <strong>simulate the environment</strong> in their mind, reason about &quot;if I'm in state X now, what's likely next?&quot; and make informed decisions. From reflexive actions to strategic planning, assuming a Markovian world simplifies the complexity. AI researchers and engineers often start with this assumption because it's mathematically tractable and often a reasonable approximation of many domains.</p>
<p><img src="https://oscarbastardo.com/mdp/figure1.png" alt="Markov Decision Process diagram"><br>
<em>Figure:</em> An illustration of an AI agent interacting with its environment, which can be modeled as a Markov process. The agent perceives the current <strong>state</strong> $S_t$ and takes an <strong>action</strong> $A_t$; the environment then transitions to a new <strong>state</strong> $S_{t+1}$ (following the Markov chain dynamics) and provides a <strong>reward</strong> $R_{t+1}$ (in purely Markov chains without actions, you can ignore the action and reward). This loop (often depicted as in the figure) highlights the Markov property: the environment's transition to $S_{t+1}$ depends only on the state $S_t$ and the agent's action, not on any earlier history.</p>
<p><a href="https://oscarbastardo.com/2021-09-26/mdp/">Source: Oscar Bastardo - Markov Decision Process</a></p>
<h2 id="5-wrap-up--next-steps">5. Wrap-Up &amp; Next Steps</h2>
<p>Markov chains are a fundamental concept in AI and many other fields because they provide a simple yet powerful way to model sequential processes. In this introductory chapter, we defined Markov chains, topics such as <strong>Hidden Markov Models (HMMs)</strong> for sequence data in AI (like speech or text), <strong>Markov Decision Processes (MDPs)</strong> for optimal decision-making in uncertain environments, and <strong>Partially Observable MDPs (POMDPs)</strong> for dealing with uncertainty in perception. All these advanced topics build on the ideas introduced here: states, transitions, and the memoryless property.</p>
<p>To reinforce these concepts, let's consider a creative real-world example of a Markov chain in action. Imagine the trending topics on a social media platform (like Twitter or TikTok). We can define states for a given topic such as: <strong>Dormant</strong> (hardly anyone is talking about it), <strong>Trending</strong> (it's gaining popularity rapidly), or <strong>Viral</strong> (it's extremely popular and widespread). On each day, the topic transitions between these states with certain probabilities. Perhaps if a topic is Trending today, there's a 50% chance it stays Trending tomorrow, a 40% chance it goes Viral, and a 10% chance it falls off to Dormant if interest dies down. This can be captured in a transition matrix. Over time, this forms a Markov chain of topic popularity. We could use it to answer questions like &quot;What's the long-term fraction of time a typical topic spends in Viral state?&quot; or &quot;What's the probability that a currently Viral meme will be Dormant in two days?&quot; – all using the computations we discussed (by looking at $T^n$ or solving for steady-state). This social network dynamics example shows how Markov chains can model everyday phenomena inhttps://claude.site/artifacts/58803752-5355-47aa-b34d-fa2fb8adeb22 AI – in this case, the behavior of users collectively driving topics between popularity states.</p>
<p>As you move forward, keep in mind how the <strong>Markov property</strong> simplifies learning and inference. In more complex models (HMMs, MDPs, etc.), we often assume an underlying Markov chain because it makes the mathematics workable and often approximates reality well. In the coming chapters, we will delve into those topics: for instance, we'll see how an HMM uses a Markov chain to model sequences with hidden states, and how solving an MDP is essentially finding an optimal policy on a Markov chain of states when actions are involved. By mastering Markov chains, you'll be well-equipped to understand and build upon these advanced AI techniques.</p>
<p>Markov chains demonstrate that sometimes <strong>memorylessness is more than enough</strong> – by focusing on &quot;now&quot;, we can predict the future and make intelligent decisions without drowning in past data. Now that you've been introduced to Markov chains, you're ready to explore the rich landscape of sequential models in AI that build on this concept. Happy learning as you step into HMMs, MDPs, POMDPs, and beyond, armed with the knowledge of Markov chains!</p>
<h2 id="a-mathematical-apendix">A. Mathematical Apendix</h2>
<p>front_end_string = '''</p>
<h3 id="a1-algebraic-derivation-neumann-series">A.1 Algebraic Derivation (Neumann Series)</h3>
<p>Consider the scalar analogy: for $\lvert x\rvert &lt; 1$, we know</p>
<p>$\frac{1}{1-x} = 1 + x + x^2 + x^3 + \dots$.</p>
<p>There is a well-known <strong>matrix version</strong> of this geometric-series identity: if $Q$ is a matrix with spectral radius $\rho(Q)&lt;1$, then</p>
<p>$(I-Q)^{-1} = \sum_{m=0}^{\infty} Q^m = I + Q + Q^2 + \dots$.</p>
<p><strong>Why?</strong> Multiply $(I-Q)$ by the finite partial sum $S_m = I + Q + \dots + Q^m$:</p>
<p>$(I - Q) S_m = (I + Q + \dots + Q^m) - (Q + Q^2 + \dots + Q^m + Q^{m+1}) = I - Q^{m+1}$.</p>
<p>As $m \to \infty$, if $\rho(Q)&lt;1$, then $Q^{m+1} \to 0$, so</p>
<p>$(I - Q)(\lim_{m\to\infty} S_m) = I$,</p>
<p>meaning $\lim_{m\to\infty} S_m$ is indeed $(I-Q)^{-1}$.</p>
<p>In the absorbing-chain scenario, $\rho(Q)&lt;1$ because &quot;once you leave the transient states, you can't come back,&quot; so $Q^m\to 0$ as $m\to\infty$. Hence the infinite sum converges to $(I-Q)^{-1}$.</p>
<h3 id="a12-markov-chain-interpretation">A.1.2 Markov Chain Interpretation</h3>
<p>When we say $N = (I - Q)^{-1} = I + Q + Q^2 + \dots$, we are summing up all possible <strong>powers</strong> of $Q$.</p>
<ul>
<li>$Q^m$ represents &quot;the probabilities of going from one transient state to another in exactly $m$ steps <strong>without being absorbed</strong>.&quot;</li>
<li>$I$ (the identity) is effectively &quot;0-step transitions&quot; (you start in state $i$ with probability 1 of being in $i$ at step zero).</li>
<li><strong>Adding</strong> them up, $\sum_{m=0}^{\infty} Q^m$ accumulates the probability—and eventually the expected <strong>number of visits</strong>—of being in each transient state at each step up to infinity.</li>
</ul>
<p>Because the chain cannot linger in transient states forever (there is always a chance to transition to an absorbing state), powers $Q^m$ become negligible for large $m$. In fact, $Q^m\to 0$, capturing that eventually you <strong>escape</strong> the transient set with probability 1. Mathematically, that ensures the infinite series converges, and <strong>algebraically</strong> that it equals $(I-Q)^{-1}$.</p>
<p>Hence, from a <strong>Markov chain perspective</strong>:</p>
<ol>
<li>$N = (I - Q)^{-1}$ <strong>sums up</strong> contributions from all possible &quot;transient circuits&quot; you might do before absorption.</li>
<li>$N_{ik}$ is the <strong>expected number of visits</strong> to transient state $k$ starting from transient state $i$.</li>
<li>When you multiply $N$ by $R$ (the transitions from transient to absorbing), you get $B = N R$, which gives <strong>the probability</strong> of ending in each absorbing state.</li>
</ol>
<p>All of this is ultimately grounded in the spectral property that $\rho(Q)&lt;1$, which ensures $(I-Q)$ is invertible and the series $\sum_{m=0}^\infty Q^m$ converges.</p>

</body>

</html>